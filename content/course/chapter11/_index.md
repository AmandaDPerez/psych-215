---
linktitle: Chapter 11
summary: Chapter 11
weight: 20
icon: book
icon_pack: fas
title: Chapter 11
date: "2018-09-09T00:00:00Z"
type: book  # Do not modify.
editor_options: 
  markdown: 
    wrap: 72
---

# Research Design

**First, watch the below video on threats to internal validity**

[![Dr. B.'s Research
Class](https://img.youtube.com/vi/o4qYHMNIR1c/0.jpg)](http://www.youtube.com/watch?v=o4qYHMNIR1c)

## The Really Bad Experiment (A Cautionary Tale)

The really bad experiment is also known as a **one-group
pretest/posttest design**, which means that there is one group of
participants who are measured on a pretest, exposed to a
treatment/intervention/change, and then measured on a posttest. Such a
design is problematic because it is vulnerable to threats to internal
validity.

## Six Potential Internal Validity Threats in One-Group, Pretest/Posttest Designs

1.  Maturation threats to internal validity
2.  History threats to internal validity
3.  Regression threats to internal validity
4.  Attrition threats to internal validity
5.  Testing threats to internal validity
6.  Instrumentation threats to internal validity

### Maturation Threats to Internal Validity

**Maturation threat:** a change in behavior that emerges spontaneously
over time. For example: Children become better and faster at solving
addition and subtraction problems as they get older, trees grow taller
with age, and over time, people tend to recover on their own from
various psychological disorders.

**Prevention:** In a really bad experiment, remember that there is only
one group of participants and they are getting the treatment. But in a
true experiment there is a comparison group in order to prevent
maturation threats.

### History Threats to Internal Validity

**History threats** result when some external or "historical" event
affects most members of the treatment group at the same time as the
treatment, so it's unclear whether the change in the DV for the
experimental group was the result of the treatment or the result of the
historical factor.

For example: Suppose you were studying the effects of meditation on
stress levels among college students, and while you were conducting the
study, a violent event occurred on the college campus at which you were
collecting your data. The meditation group did not show significant
decreases in stress levels as expected, but was that because the
treatment wasn't effective? Perhaps it was effective but the campus
violence raised people's stress levels, which made it look like it was
not effective.

Another Example: A study is underway in 2001 in which the DOD
(Department of Defense) is comparing three recruiting approaches, (1)
the standard "wait for the recruit to walk into the office" approach,
(2) cash payment for enlisting and bringing in one additional recruit,
and (3) cash payments for enlistment. The attacks on the World Trade
Center occur. Enlistment goes way up. The true experiment protects
against this threat because of random assignment to the three
treatments. The increase will be greater for all treatments, which will
still permit the DOD to determine which produces more recruits.

### Regression Threats to Internal Validity

**Regression threat** (aka regression to the mean): a statistical
concept in which extremely low or extremely high performance at Time 1
is likely to be less extreme at Time 2 (i.e., closer to average). For
example: If you're in a really good mood or a really bad mood, you will
probably be in a more moderate mood tomorrow due to regression.

**Regression and internal validity:** Regression threats only occur in
pretest/posttest designs. Specifically, they only occur when a group has
an extreme pretest score (high or low). You can anticipate that the
scores of those participants will regress toward the mean at posttest.

**Prevention:** Regression threats can be avoided by using comparison
groups and inspecting the results.

### Attrition Threats to Internal Validity

**Attrition threat:** a reduction in participant numbers from pretest to
posttest. Attrition is only problematic when it is systematic.

Prevention: One way to prevent attrition is that when participants drop
out of a study, the researcher removes their scores from the pretest
average. Another approach is to look at the pretest scores of dropouts.
If they have extreme scores, then they are more likely to threaten
internal validity than if they have more moderate scores.

### Testing Threats to Internal Validity

**Testing threat:** a type of order effect in which there is a change in
participants as a result of experiencing the DV (the test) more than
once. Their scores might go up due to practice (practice effect), or
their scores might go down due to fatigue (fatigue effect). Testing
threats affect internal validity because it's not clear if the treatment
caused the change in the DV or whether practice or fatigue did.

Prevention: One way to prevent testing threats is not to use a pretest
(posttest-only design). Another way is to use alternative forms of the
test at pretest and posttest. Having a comparison group is also helpful.
You can rule out testing threats if both groups take the pretest and the
posttest, but the treatment group exhibits a larger change than the
comparison group.

### Instrumentation Threats to Internal Validity

**Instrumentation threat** (aka instrumentation decay): occurs when a
measuring instrument changes over time. For example: observers change
their observation criteria over time, or a researcher uses different
forms of a test at pretest and posttest and they're not equivalent
forms.

Prevention: One way to prevent instrumentation threats is to use a
posttest-only design. However, if you need a pretest/posttest design,
you should make sure that your pretest and posttest forms are
equivalent. In terms of making observations, you might retrain your
observers throughout the study. Finally, you might counterbalance the
order of the pretest and posttest forms, such that some participants get
Form A at pretest and some get Form B, and then they get the other form
at posttest.

**Instrumentation vs. testing threats:** In an instrumentation threat,
the measuring instrument has changed from Time 1 to Time 2, whereas in a
testing threat, the participant changes over the period between Time 1
and Time 2.

## Combined Threats

Sometimes in a pretest/posttest design two types of threats to internal
validity might work together.

**Selection-history threat:** suppose students at one university were in
your treatment group and students at another university were in your
control group in a study of the effects of meditation on stress.
However, during the course of the study, a stressful event occurs on one
of the campuses.

**Selection-attrition threat:** Let's say that participants in one group
have to travel 1 mile for the study, and participants in the other group
have to travel 20 miles for the study. You might have more attrition in
the 20-mile group due to the distance from the lab, so you could be sure
if differences between groups were due to the IV or the distance and
attrition.

## Three Potential Internal Validity Threats in Any Study

Observer bias, demand characteristics, and placebo effects are three
more potential threats to internal validity, and they can occur, not
only in the very bad experiment (one-group pretest/posttest design), but
also in experiments that have a comparison group.

### Observer bias

This is bias caused by researchers' expectations influencing how they
interpret the results. Example: Dr. Yuki might be a biased observer of
her patients' depression: She expects to see her patients improve,
whether they do or do not. Nikhil may be a biased observer of his
campers: He may expect the low-sugar diet to work, so he views the boys'
posttest behavior more positively.

Although comparison groups can prevent many threats to internal
validity, they do not necessarily control for observer bias. Even if Dr.
Yuki used a no-therapy comparison group, observer bias could still
occur: If she knew which participants were in which group, her biases
could lead her to see more improvement in the therapy group than in the
comparison group.

### Demand characteristics

This is bias that occurs when participants figure out what a research
study is about and change their behavior in the expected direction. In
order to control for observer bias and demand characteristics, a
researcher can conduct a double-blind study, which is designed so that
neither the participants nor the experimenters working with the
participants know who is in the treatment group and who is in the
control group. If a double-study isn't feasible, then an acceptable
alternative is a masked design (aka blind design; participants know
which group they're in but observers don't).

### Placebo effect

This effect is present when people receive a treatment and improve, but
only because they believe they are receiving a valid or effective
treatment. For example: Participants are told they are receiving a new
therapy, pill, or injection, but in fact it's missing the active
ingredient, or they may be told they are getting a new type of therapy
but in fact simply chatted with someone and didn't get any therapy.
Nonetheless, participants may improve because they thought they had had
therapy. It's important to note that placebo effects aren't imaginary.
In fact, placebos can be strong treatments.

Designing studies to rule out the placebo effect: In order to rule out
the placebo effect, a special comparison group is used that is receiving
the placebo therapy or placebo medication, but neither the people
working with the participants nor the participants know who is in which
group (double-blind placebo control study).

## Interrogating Null Effects: What If the Independent Variable Does Not Make a Difference?

## Perhaps there is not enough between-groups difference.

### Ceiling and Floor Effects

Ceiling effect: the participants' scores on the DV are clustered at the
high end (for example, giving college students a simple addition test).
Example: Suppose the researcher manipulated anxiety by telling the
groups they were about to receive an electric shock. The low-anxiety
group was told to expect a 10-volt shock, the medium-anxiety group was
told to expect a 50-volt shock, and the high-anxiety group was told to
expect a 100-volt shock. This manipulation would probably result in a
ceiling effect because expecting any amount of shock would cause
anxiety, regardless of the shock's intensity. As a result, the various
levels of the independent variable would appear to make no difference
Floor effect: the participants' scores on the DV are clustered at the
low end. Example: If a researcher really did manipulate the independent
variable by giving people \$0.00, \$0.25, or \$1.00, that would be a
floor effect because these three amounts are all low---they're squeezed
close to a floor of \$0.00 Ceilings, floors, and IVs: sometimes ceiling
and floor effects can be the result of a problematic IV, as in the money
and mood study, in which all three levels of the IV were very low
amounts of money: none, 25 cents, or a dollar. Ceilings, floors, and
DVs: poorly designed DVs can also cause ceiling and floor effects.

**Manipulation Checks Help Detect Weak Manipulations, Ceilings, and
Floors**

Manipulation check: a second DV included in a study to make sure the IV
manipulation worked.

### Design Confounds Acting in Reverse

Although confounds usually threaten internal validity, they can apply to
null effects.

Example: In a GRE study, perhaps that test-prep group was also under
additional pressure to perform well on the GRE. As a result, they were
actually receiving test prep and pressure, while the no-test prep group
didn't have test prep or pressure.

The added pressure that was applied is considered a confound. However,
it didn't work in favor of the test-prep group; it worked against them
by lowering their scores.

## Perhaps within-groups variability obscured the group differences.

### Measurement Error

Measurement error: any factor that can inflate or deflate a person's
true score on the DV. Example: a man who is 160 centimeters tall might
be measured at 160.5 cm because of the angle of vision of the person
using the meter stick, or he might be recorded as 159.5 cm because he
slouched a bit. The goal is to keep measurement error as small as
possible.

Solutions for reducing measurement error

Use reliable, precise measurements: measurement errors are reduced when
researchers use measurement tools that are reliable (internal,
interrater, and test/retest) and that are valid (i.e., have good
construct validity).

Measure more instances: if researcher can't find a measurement tool
that's reliable and valid, then the best alternative is to measure a
larger sample of participants. Random errors will cancel each other out
with more people in the sample.

### Individual Differences

Another source of within-group variability is individual differences.
Some people are faster runners than others, some are smarter, some are
funnier, and so on.

There are two solutions for reducing the effects of individual
differences.

1.  Change the design: use a within-groups design instead of an
    independent-groups design . When you do this, then each person
    receives both levels of the IV, and individual differences are
    controlled for. It's easier to see the effect of the IV when
    individual differences aren't obscuring between-groups differences.
    You can also use a matched-groups design. Pairs of participants are
    matched on an individual differences variable, and it's easier to
    see the effects of the IV.

2.  Add more participants: if it's not feasible to change the design to
    a within-groups or matched-groups design, then try adding more
    participants. This will lessen the effect that any one participant
    has on the group average.

### Situation Noise

**Situation noise:** external distractions of any kind that obscure
between-groups difference and cause variability within groups.

Example: This includes smells, sights, and sounds that might distract
participants and increase within-groups variability; it adds
unsystematic variability to each group situation by controlling the
surroundings of an experiment that might affect the DV.

Example: testing participants in a quiet room with no outside odors, and
so on.

## Another Name for These Solutions: Power

**Power** is an aspect of statistical validity. For example: If GRE prep
courses really work to increase GRE scores, then the study will detect
this difference.

Studies with more power can detect even small effects. We can illustrate
this with a light as an analogy for statistical power. Not having very
much power is like trying to find an object in a room lit by a candle.
You probably wouldn't be able to find small objects by candlelight, but
you could find larger objects. Having high statistical power is like
having a flashlight. It's easy to find even small objects in a room lit
with a bright flashlight.

## Null Effects May Be Published Less Often

A null effect in a study can be just as interesting as a group
difference when studies are conducted with adequate powers. Null effects
are reported in scholarly literature. However, in the popular media, you
will rarely find null results. The popular press focuses on group
differences.
